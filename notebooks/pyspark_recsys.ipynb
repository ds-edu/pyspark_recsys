{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7015cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "\n",
    "import random\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, DateType, FloatType, TimestampType, ArrayType\n",
    "from pyspark.sql.functions import col, sum, isnan, isnull, isnotnull, when, countDistinct, count, regexp_replace, split, month, year, size, element_at, struct, trim, avg, expr, lit\n",
    "from pyspark.sql.functions import concat, concat_ws, rand, collect_list, struct\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75621ae6",
   "metadata": {},
   "source": [
    "#### File sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2a5aef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliances\n",
    "url_meta = 'https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/raw/meta_categories/meta_Appliances.jsonl.gz'\n",
    "url_ratings = 'https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/raw/review_categories/Appliances.jsonl.gz'\n",
    "\n",
    "\n",
    "# # Electronics\n",
    "# url_meta = 'https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/raw/meta_categories/meta_Electronics.jsonl.gz'\n",
    "# url_ratings = 'https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/raw/review_categories/Electronics.jsonl.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f485a1",
   "metadata": {},
   "source": [
    "#### Generating reduced size dummy test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d49c892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_sampling(url, chunk_size, sample_size, random_state=None):\n",
    "    \"\"\"\n",
    "    Reads a JSON file in chunks, samples from each chunk, and returns a combined sample.\n",
    "\n",
    "    Args:\n",
    "        url (str): JSON file url\n",
    "        chunk_size (int): Number of rows to read in each chunk.\n",
    "        sample_size (int): Total number of rows to sample.\n",
    "        random_state (int, optional): Seed for random sampling. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the sampled rows.\n",
    "    \"\"\"\n",
    "    if random_state is not None:\n",
    "        random.seed(random_state)\n",
    "\n",
    "    sampled_chunks = []\n",
    "    total_rows = 0\n",
    "\n",
    "    for chunk in pd.read_json(url, lines=True, chunksize=chunk_size):\n",
    "       \n",
    "        chunk_sample_size = min(sample_size - total_rows, len(chunk))\n",
    "        if chunk_sample_size > 0:\n",
    "            sampled_chunk = chunk.sample(n=chunk_sample_size, random_state=random_state)\n",
    "            sampled_chunks.append(sampled_chunk)\n",
    "            total_rows += chunk_sample_size\n",
    "        if total_rows >= sample_size:\n",
    "            break\n",
    "\n",
    "    return pd.concat(sampled_chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179dfb94",
   "metadata": {},
   "source": [
    "#### Reduce samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcf83f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute UDF and save dummy files locally\n",
    "\n",
    "df_reviews = read_json_sampling(url_ratings, 100, 500, 33)\n",
    "df_meta = read_json_sampling(url_meta, 100, 500, 33)\n",
    "\n",
    "df_reviews.to_json('../data/reduced_app_reviews.jsonl.gz', orient='records', lines='true', compression='gzip')\n",
    "df_meta.to_json('../data/reduced_app_meta.jsonl.gz', orient='records', lines='true', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf925e3",
   "metadata": {},
   "source": [
    "#### Spark sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491e9ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "25/05/22 12:42:33 WARN Utils: Your hostname, DPC resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/05/22 12:42:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/22 12:42:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 47636)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/edu/anaconda3/lib/python3.12/socketserver.py\", line 318, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/edu/anaconda3/lib/python3.12/socketserver.py\", line 349, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/edu/anaconda3/lib/python3.12/socketserver.py\", line 362, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/edu/anaconda3/lib/python3.12/socketserver.py\", line 761, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/edu/DataScience/CapstoneProjects/Spark_RecSys/venv_spark/lib/python3.12/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/home/edu/DataScience/CapstoneProjects/Spark_RecSys/venv_spark/lib/python3.12/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/home/edu/DataScience/CapstoneProjects/Spark_RecSys/venv_spark/lib/python3.12/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/edu/DataScience/CapstoneProjects/Spark_RecSys/venv_spark/lib/python3.12/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.driver.memory\", \"60g\") \\\n",
    "    .appName(\"RecSys2\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05279bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.caseSensitive', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83bd1b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.255.255.254:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>RecSys2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa25671fe90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd9429f",
   "metadata": {},
   "source": [
    "#### Loading files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b26454e",
   "metadata": {},
   "source": [
    "###### Via URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98b0a75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df_meta = spark.read.json(url_meta)\n",
    "# # df_meta = spark.read.json(url_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc8632a",
   "metadata": {},
   "source": [
    "###### Locally stored files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de2bac22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/22 12:43:06 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "# Reduced files\n",
    "\n",
    "df_meta = spark.read.json('../data/reduced_app_meta.jsonl.gz'  )\n",
    "df_ratings = spark.read.json('../data/reduced_app_reviews.jsonl.gz' )\n",
    "\n",
    "#Full files\n",
    "\n",
    "# df_meta = spark.read.json('data/meta_Appliances.jsonl.gz')\n",
    "# df_ratings = spark.read.json('data/Appliances.jsonl.gz')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43dbe41",
   "metadata": {},
   "source": [
    "### Cleaning Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9eba431f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata shape: (500, 14)\n"
     ]
    }
   ],
   "source": [
    "print(f'Metadata shape: ({df_meta.count()}, {len(df_meta.columns)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c61a307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_meta.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47066274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['average_rating',\n",
       " 'bought_together',\n",
       " 'categories',\n",
       " 'description',\n",
       " 'details',\n",
       " 'features',\n",
       " 'images',\n",
       " 'main_category',\n",
       " 'parent_asin',\n",
       " 'price',\n",
       " 'rating_number',\n",
       " 'store',\n",
       " 'title',\n",
       " 'videos']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meta.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4ac1e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns in meta that are not needed in the analysis\n",
    "\n",
    "cols_to_drop = ['images', 'videos', 'bought_together', 'price', 'rating_number', 'average_rating']\n",
    "\n",
    "df_meta = df_meta.drop(*cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a5228aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-----+-----+\n",
      "|main_category|parent_asin|store|title|\n",
      "+-------------+-----------+-----+-----+\n",
      "|            1|          0|    6|    0|\n",
      "+-------------+-----------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count null or nan columns that don't contain arrays\n",
    "\n",
    "cols_non_arr = [c for c, t in df_meta.dtypes if (t.startswith('array')==False) and (t.startswith('struct') == False) ] \n",
    "\n",
    "df_meta.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in cols_non_arr]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3260639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate % of blanks in columns\n",
    "\n",
    "# for c in cols_non_arr:\n",
    "    \n",
    "#     print(f'Percentage of blank rows in {c}: {round(100 * df_meta.where(col(c).isNull() | isnan(c)).count() / df_meta.count(), 3)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f86230",
   "metadata": {},
   "source": [
    "### Cleaning Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2775d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings shape: (500, 10)\n"
     ]
    }
   ],
   "source": [
    "print(f'Ratings shape: ({df_ratings.count()}, {len(df_ratings.columns)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c89b92bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asin',\n",
       " 'helpful_vote',\n",
       " 'images',\n",
       " 'parent_asin',\n",
       " 'rating',\n",
       " 'text',\n",
       " 'timestamp',\n",
       " 'title',\n",
       " 'user_id',\n",
       " 'verified_purchase']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratings.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b1398cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- helpful_vote: long (nullable = true)\n",
      " |-- images: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- attachment_type: string (nullable = true)\n",
      " |    |    |-- large_image_url: string (nullable = true)\n",
      " |    |    |-- medium_image_url: string (nullable = true)\n",
      " |    |    |-- small_image_url: string (nullable = true)\n",
      " |-- parent_asin: string (nullable = true)\n",
      " |-- rating: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- verified_purchase: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ratings.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55104024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ratings.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a284fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['title', 'text', 'images', 'helpful_vote', 'verified_purchase', 'timestamp' ]\n",
    "df_ratings = df_ratings.drop(*cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b388ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols_non_arr = [c for c, t in df_ratings.dtypes if (t.startswith('array')==False) and (t.startswith('struct') == False) ] \n",
    "\n",
    "# for c in cols_non_arr:\n",
    "#     print(f'Percentage of blank rows in {c}: {round(100 * df_ratings.where(col(c).isNull() | isnan(c)).count() / df_ratings.count(), 3)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59160b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function\n",
    "\n",
    "def meta_lookup(parent_asin:str):\n",
    "\n",
    "    return df_meta.filter(col('parent_asin') == parent_asin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8027ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function\n",
    "\n",
    "def ratings_lookup(parent_asin:str):\n",
    "\n",
    "    return df_ratings.filter(col('parent_asin') == parent_asin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43f7506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x= meta_lookup('B00Q4X2FSM')\n",
    "# x.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9fc2ed4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Try\n",
    "# df_meta.filter(col('parent_asin') == 'B07S9DJ2S2').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e60cebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Try: \n",
    "# col_custom = list(F.col(f).alias(f) for f in  df_meta.columns) + list(map(lambda f: F.col(\"details\").getItem(f).alias(str(f)), [\"Brand\", \"Manufacturer\"]))\n",
    "# col_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a551187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Try\n",
    "\n",
    "# df_meta.select(col_custom).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5eb442db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count blank [stores]\n",
    "\n",
    "df_meta.filter(col('store').isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bc4e8a",
   "metadata": {},
   "source": [
    "### FE: Create column -> Maker : Extract from Brand/Manufacturer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a3e7c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF for getting brand/manufacturer/distributor from details \n",
    "\n",
    "def get_maker(dict_col, key1, key2):\n",
    "    if dict_col is None:\n",
    "        return None\n",
    "    if key1 in dict_col and dict_col[key1]:\n",
    "        return dict_col[key1]\n",
    "    elif key2 in dict_col:\n",
    "        return dict_col[key2]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "get_maker_udf = F.udf(get_maker, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88093bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call udf get_maker to fill [details] with brand or manufacturer info\n",
    "\n",
    "df_meta = df_meta.withColumn(\n",
    "    \"maker\",\n",
    "    get_maker_udf(df_meta[\"details\"], \\\n",
    "                F.lit(\"Brand\"), \\\n",
    "                F.lit(\"Manufacturer\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b8c3404c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show null values in maker column\n",
    "\n",
    "df_meta.where(col('maker').isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e7f1a9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since [store] values will be used for maker, ensure there'e no blank values \n",
    "df_meta = df_meta.na.fill({'store':'Unknown'})\n",
    "\n",
    "# Checkpoint: Assert no remaining nulls in [store]\n",
    "assert df_meta.filter(col('store').isNull()).count() == 0, 'Blank values in store. Check imputation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2be98640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill null columns in [maker] with values from [store]\n",
    "from pyspark.sql.functions import coalesce\n",
    "df_meta = df_meta.withColumn('maker', coalesce('maker', 'store') )\n",
    "\n",
    "# Checkpoint: Assert no remaining nulls in [maker]\n",
    "assert df_meta.filter(col('maker').isNull()).count() == 0, 'Blank values in maker. Check imputation'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4e93ef",
   "metadata": {},
   "source": [
    "### FE: Create column -> Feature_Group : Concatenation of different columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ecbaccff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column with the concatenated values of the columns of interest\n",
    "cols_of_interest = ['parent_asin', 'main_category', 'maker', 'title' ] # Add desc and details later\n",
    "df_meta = df_meta.withColumn('feature_group', concat_ws(' ', *cols_of_interest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2684bcb9",
   "metadata": {},
   "source": [
    "### Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aa7c61a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/edu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/edu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/edu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF, MinHashLSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7746f8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d245979b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF for preprocessing text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Register the UDF\n",
    "preprocess_text_udf = F.udf(preprocess_text, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3bdf745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the UDF to the 'text' column\n",
    "df_meta = df_meta.withColumn('feat_preproc', preprocess_text_udf(df_meta['feature_group']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1590c46e",
   "metadata": {},
   "source": [
    "#### Tokenizing the text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4ac5b074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the preprocessed text \n",
    "tokenizer = Tokenizer(inputCol='feat_preproc', outputCol='feat_tokens')\n",
    "df_tokenized = tokenizer.transform(df_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e95c9e",
   "metadata": {},
   "source": [
    "#### Add Extra Lemmatization step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1a8d0922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF: Lemmatize tokens\n",
    "import numpy as np\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Lemmatize the input tokens.\n",
    "    Args:\n",
    "        tokens (list): List of tokens to lemmatize.\n",
    "    Returns:\n",
    "        list: List of lemmatized tokens.\n",
    "    \"\"\"\n",
    "    if tokens is None:\n",
    "        return None\n",
    "\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens if token not in stop_words]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Register the UDF\n",
    "lemmatize_tokens_udf = F.udf(lemmatize_tokens, ArrayType(StringType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e01b170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the UDF to the 'tokens' column\n",
    "\n",
    "df_lemmatized = df_tokenized.withColumn('feat_lemma', lemmatize_tokens_udf(df_tokenized['feat_tokens']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d47c16",
   "metadata": {},
   "source": [
    "#### Creating Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "43005d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctvec_file = None\n",
    "hashtf_file = None\n",
    "tf_ctvector = None\n",
    "tf_vector = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2619e54",
   "metadata": {},
   "source": [
    "##### A. Using Hashing Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "16f01219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Hashing Term Frequency to create feature vectors - Similar to TF-IDF but without the IDF component\\n# The HashingTF class is used to convert a sequence of terms into a feature vector using the hashing trick.\\n# HashingTF is a feature transformer that maps a sequence of terms to their term frequencies using the hashing trick.\\n# It is a fast and efficient way to convert text data into numerical feature vectors.\\n# The numFeatures parameter specifies the number of features to create.\\n# The output is a sparse vector of term frequencies, where each index corresponds to a hashed term.\\n\\n\\nhashingTF = HashingTF(inputCol=\"feat_lemma\", outputCol=\"feat_vectors\" ) #, numFeatures=2048)\\ntf_vector = hashingTF.transform(df_lemmatized)\\n\\ntf_vector.cache()\\ntf_vector.select(col(\\'feat_vectors\\')).show(5)\\n\\nctvec_file = None\\nhashtf_file = \\'./data/similarity_hashtf.csv\\' \\n\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Hashing Term Frequency to create feature vectors - Similar to TF-IDF but without the IDF component\n",
    "# The HashingTF class is used to convert a sequence of terms into a feature vector using the hashing trick.\n",
    "# HashingTF is a feature transformer that maps a sequence of terms to their term frequencies using the hashing trick.\n",
    "# It is a fast and efficient way to convert text data into numerical feature vectors.\n",
    "# The numFeatures parameter specifies the number of features to create.\n",
    "# The output is a sparse vector of term frequencies, where each index corresponds to a hashed term.\n",
    "\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"feat_lemma\", outputCol=\"feat_vectors\" ) #, numFeatures=2048)\n",
    "tf_vector = hashingTF.transform(df_lemmatized)\n",
    "\n",
    "tf_vector.cache()\n",
    "tf_vector.select(col('feat_vectors')).show(5)\n",
    "\n",
    "ctvec_file = None\n",
    "hashtf_file = './data/similarity_hashtf.csv' \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2002abfc",
   "metadata": {},
   "source": [
    "##### B. Using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "03ff10f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[categories: array<string>, description: array<string>, details: struct<Access Location:string,Age Range (Description):string,Air Flow Capacity:string,Airflow Displacement:string,Alarm:string,Amperage Capacity:string,Annual Energy Consumption:string,Are Batteries Included:string,Assembly required:string,Backlight:string,Batteries:string,Batteries Included?:string,Batteries Required?:string,Batteries required:string,Battery Cell Type:string,Best Sellers Rank:struct<Amazon Renewed:bigint,Appliances:bigint,Beverage Refrigerators:bigint,Chest Freezers:bigint,Clothes Dryer Replacement Lint Screens:bigint,Clothes Dryer Replacement Vents:bigint,Clothes Washer Replacement Doors:bigint,Clothes Washer Replacement Drain Pumps:bigint,Clothes Washing Machines:bigint,Commercial Bag Sealers:bigint,Commercial Shrink Wrappers:bigint,Compact Refrigerators:bigint,Cooktop Parts & Accessories:bigint,Cooktops:bigint,Dishwasher Parts & Accessories:bigint,Dishwasher Replacement Baskets:bigint,Dishwasher Replacement Hoses:bigint,Disposable Coffee Filters:bigint,Dryer Replacement Parts:bigint,Espresso Steaming Pitchers:bigint,Freezer Parts & Accessories:bigint,Health & Household:bigint,Home & Kitchen:bigint,Humidifier Accessories:bigint,Humidifier Humidity Meters:bigint,Humidifier Parts & Accessories:bigint,Humidifier Replacement Wicks:bigint,Ice Makers:bigint,In-Refrigerator Water Filters:bigint,Industrial & Scientific:bigint,Jugs:bigint,Kitchen & Dining:bigint,Oven Parts & Accessories:bigint,Paper & Plastic Household Supplies:bigint,Parts & Accessories:bigint,Permanent Coffee Filters:bigint,Portable Clothes Washing Machines:bigint,Portable Dryers:bigint,Range Accessories:bigint,Range Hood Filters:bigint,Range Hoods:bigint,Range Parts & Accessories:bigint,Range Replacement Burners:bigint,Range Replacement Drip Pans:bigint,Range Replacement Knobs:bigint,Refrigerator Egg Trays:bigint,Refrigerator Parts & Accessories:bigint,Refrigerator Replacement Bins:bigint,Refrigerator Replacement Handles:bigint,Refrigerator Replacement Ice Makers:bigint,Refrigerator Replacement Motors:bigint,Refrigerator Replacement Shelves:bigint,Refrigerators:bigint,Reusable Coffee Filters:bigint,Tools & Home Improvement:bigint,Washer Parts & Accessories:bigint>,Brand:string,Brand Name:string,Bulb Base:string,Bulb Shape Size:string,Burner type:string,Capacity:string,Capacity Name:string,Capacity Total:string,Care instructions:string,Center To Center Spacing:string,Certification:string,Closure Type:string,Color:string,Color Temperature:string,Compatible Device:string,Compatible Devices:string,Configuration:string,Connectivity Technology:string,Connector Type:string,Contains Liquid Contents:string,Control Console:string,Control Method:string,Control Type:string,Controller Type:string,Controls Type:string,Cooling Power:string,Cooling Vents:string,Cord Length:string,Country of Origin:string,Cycle Options:string,Date First Available:string,Defrost:string,Defrost System:string,Department:string,Diameter:string,Domestic Shipping:string,Door Hinges:string,Door Material Type:string,Drawer Type:string,Duration:string,Efficiency:string,Energy Star:string,Exterior:string,Exterior Finish:string,External Testing Certification:string,Finish:string,Finish Type:string,Finish types:string,Floor Area:string,Flow Rate:string,Form Factor:string,Freezer Capacity:string,Fuel Type:string,Fuel type:string,Handle Material:string,Handle Type:string,Handle/Lever Placement:string,Heater Surface Material:string,Heating Elements:string,Heating Method:string,Horsepower:string,Hose Length:string,Human Interface Input:string,Ignition System Type:string,Import:string,Incandescent Equivalent Wattage:string,Included Components:string,Installation Method:string,Installation Type:string,International Shipping:string,Is Discontinued By Manufacturer:string,Is Dishwasher Safe:string,Is Microwaveable:string,Item Dimensions LxWxH:string,Item Form:string,Item Package Dimensions L x W x H:string,Item Package Quantity:string,Item Weight:string,Item model number:string,Light Color:string,Light Type:string,Lighting:string,Manufacturer:string,Manufacturer Part Number:string,Manufacturer Warranty:string,Material:string,Material Type:string,Max Spin Speed:string,Maximum Power:string,Maximum Pressure:string,Maximum Rotational Speed:string,Measurement Accuracy:string,Measurement System:string,Metal Type:string,Model Info:string,Model Name:string,Mounting Type:string,National Stock Number:string,Noise:string,Noise Level:string,Number Of Pieces:string,Number of Doors:string,Number of Handles:string,Number of Items:string,Number of Pieces:string,Number of Sets:string,Number of pieces:string,Operation Mode:string,Option Cycles:string,Other display features:string,Outside Diameter:string,Package Dimensions:string,Package Type:string,Package Weight:string,Part Number:string,Pattern:string,Position:string,Power Source:string,Pre-printed:string,Product Benefits:string,Product Care Instructions:string,Product Dimensions:string,Recommended Uses For Product:string,Refrigerant:string,Refrigerator Fresh Food Capacity:string,Reusability:string,Room Type:string,Runtime:string,Scent:string,Seasonal Energy Efficiency Ratio (SEER):string,Shape:string,Shelf Type:string,Shelves:string,Size:string,Sound Level:string,Special Feature:string,Special Features:string,Specification Met:string,Speed:string,Sport:string,Standard Cycles:string,Style:string,Surface Recommendation:string,Theme:string,Thickness:string,Thread Size:string,Thread Type:string,Type of Bulb:string,Unit Count:string,Upper Temperature Rating:string,Usage:string,Vehicle Service Type:string,Voltage:string,Warranty Description:string,Wattage:string,Wheel Width:string,With Lid:string>, features: array<string>, main_category: string, parent_asin: string, store: string, title: string, maker: string, feature_group: string, feat_preproc: string, feat_tokens: array<string>, feat_lemma: array<string>, feat_vectors: vector]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "# Apply CountVectorizer to count the occurrences of each phrase\n",
    "countvec = CountVectorizer(inputCol=\"feat_lemma\", outputCol=\"feat_vectors\")\n",
    "\n",
    "countvec_model = countvec.fit(df_lemmatized)\n",
    "tf_ctvector = countvec_model.transform(df_lemmatized)\n",
    "\n",
    "hashtf_file = None\n",
    "ctvec_file = './data/similarity_ctvec.csv'\n",
    "\n",
    "tf_ctvector.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f42a639e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------------------+--------------------+-----------+------------------+--------------------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|          categories|         description|             details|              features|       main_category|parent_asin|             store|               title|             maker|       feature_group|        feat_preproc|         feat_tokens|          feat_lemma|        feat_vectors|\n",
      "+--------------------+--------------------+--------------------+----------------------+--------------------+-----------+------------------+--------------------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|[Appliances, Part...|                  []|{NULL, NULL, NULL...|                    []|Tools & Home Impr...| B07CPY3X2X|            UMTELE|Compatible with L...|            UMTELE|B07CPY3X2X Tools ...|b07cpy3x2x tools ...|[b07cpy3x2x, tool...|[b07cpy3x2x, tool...|(3054,[0,1,2,4,6,...|\n",
      "|[Appliances, Part...|[8268961 Dishwash...|{NULL, NULL, NULL...|  [8268961 2pcs Dis...|Tools & Home Impr...| B09JYZZ8NQ|           GZsenwo|8268961 Dishwashe...|           GZsenwo|B09JYZZ8NQ Tools ...|b09jyzz8nq tools ...|[b09jyzz8nq, tool...|[b09jyzz8nq, tool...|(3054,[0,1,2,3,17...|\n",
      "|[Appliances, Part...|                  []|{NULL, NULL, NULL...|                    []|Tools & Home Impr...| B0824J3GGJ|                WY|Dryer Belt Replac...|                WY|B0824J3GGJ Tools ...|b0824j3ggj tools ...|[b0824j3ggj, tool...|[b0824j3ggj, tool...|(3054,[0,1,2,3,7,...|\n",
      "|[Appliances, Part...|[Fit Models: RF22...|{NULL, NULL, NULL...|  [DA97-12942A This...|Tools & Home Impr...| B09Y5T57KX|           TNITRIB|Optimize and Upgr...|           TNITRIB|B09Y5T57KX Tools ...|b09y5t57kx tools ...|[b09y5t57kx, tool...|[b09y5t57kx, tool...|(3054,[0,1,2,11,1...|\n",
      "|[Appliances, Dish...|[Get better resul...|{NULL, NULL, NULL...|                    []|          Appliances| B0052EK0MW|        KitchenAid|KitchenAid Superb...|        KitchenAid|B0052EK0MW Applia...|b0052ek0mw applia...|[b0052ek0mw, appl...|[b0052ek0mw, appl...|(3054,[5,18,29,31...|\n",
      "|[Appliances, Part...|[Large Capacity E...|{NULL, NULL, NULL...|  [Auto Rolling Egg...|Tools & Home Impr...| B0B2NP5QQT|             TOGOO|Large Capacity Eg...|             TOGOO|B0B2NP5QQT Tools ...|b0b2np5qqt tools ...|[b0b2np5qqt, tool...|[b0b2np5qqt, tool...|(3054,[0,1,2,6,35...|\n",
      "|[Appliances, Part...|                  []|{NULL, NULL, NULL...|[【PARTS NUMBER】Th...|          Appliances| B07W42P978|         AMI PARTS|WD12X10327 Rack R...|         AMI PARTS|B07W42P978 Applia...|b07w42p978 applia...|[b07w42p978, appl...|[b07w42p978, appl...|(3054,[5,9,13,28,...|\n",
      "|[Appliances, Part...|                  []|{NULL, NULL, NULL...|[【Worth buying】Su...|Tools & Home Impr...| B09XQT8Q9X|           UPTTHOW|UPTTHOW Upgraded ...|           UPTTHOW|B09XQT8Q9X Tools ...|b09xqt8q9x tools ...|[b09xqt8q9x, tool...|[b09xqt8q9x, tool...|(3054,[0,1,2,7,9,...|\n",
      "|[Appliances, Part...|[Brand new dryer ...|{NULL, NULL, NULL...|                    []|Tools & Home Impr...| B00IN9AGAE|                GE|Clothes Dryer Dru...|               RPI|B00IN9AGAE Tools ...|b00in9agae tools ...|[b00in9agae, tool...|[b00in9agae, tool...|(3054,[0,1,2,10,3...|\n",
      "|[Appliances, Part...|[SPECIFICATIONS, ...|{NULL, NULL, NULL...|  [VULCAN-HART, Ove...|Industrial & Scie...| B00NXW96TS|       Vulcan Hart|Vulcan Hart VULCA...|       Vulcan Hart|B00NXW96TS Indust...|b00nxw96ts indust...|[b00nxw96ts, indu...|[b00nxw96ts, indu...|(3054,[17,22,40,5...|\n",
      "|[Appliances, Part...|[This part shines...|{NULL, NULL, NULL...|  [Replacement ice ...|Tools & Home Impr...| B07BBF44DQ|            Siwdoy|Siwdoy 4389102 Co...|            Siwdoy|B07BBF44DQ Tools ...|b07bbf44dq tools ...|[b07bbf44dq, tool...|[b07bbf44dq, tool...|(3054,[0,1,2,3,12...|\n",
      "|[Appliances, Part...|[A washing machin...|{NULL, NULL, NULL...|  [✫ Model Number 4...|Tools & Home Impr...| B07VVR2TNH|       Repairwares|Repairwares Washi...|       Repairwares|B07VVR2TNH Tools ...|b07vvr2tnh tools ...|[b07vvr2tnh, tool...|[b07vvr2tnh, tool...|(3054,[0,1,2,11,2...|\n",
      "|[Appliances, Part...|                  []|{NULL, NULL, NULL...|                    []|Tools & Home Impr...| B078QB5M4B|After Market Parts|Compatible Oven o...|After Market Parts|B078QB5M4B Tools ...|b078qb5m4b tools ...|[b078qb5m4b, tool...|[b078qb5m4b, tool...|(3054,[0,1,2,9,12...|\n",
      "|[Appliances, Part...|[Innovative Premi...|{NULL, NULL, NULL...| [【Strong Water Ab...|         Amazon Home| B09XQTY1X8|            TAUPTT|TAUPTT Refrigerat...|            TAUPTT|B09XQTY1X8 Amazon...|b09xqty1x8 amazon...|[b09xqty1x8, amaz...|[b09xqty1x8, amaz...|(3054,[0,6,8,11,1...|\n",
      "|[Appliances, Part...|[Compatible with ...|{NULL, NULL, NULL...| [【Upgraded to Foo...|Tools & Home Impr...| B09F2Y7CLZ|              LXun|Lxun Upgraded AAP...|              LXun|B09F2Y7CLZ Tools ...|b09f2y7clz tools ...|[b09f2y7clz, tool...|[b09f2y7clz, tool...|(3054,[0,1,2,6,12...|\n",
      "|[Appliances, Part...|                  []|{NULL, NULL, NULL...|                    []|Tools & Home Impr...| B07Q24SNJ2|          Abutilon|Abutilon Dishwash...|          Abutilon|B07Q24SNJ2 Tools ...|b07q24snj2 tools ...|[b07q24snj2, tool...|[b07q24snj2, tool...|(3054,[0,1,2,3,18...|\n",
      "|[Appliances, Part...| [Whirlpool Igniter]|{NULL, NULL, NULL...|  [This is a Genuin...|Tools & Home Impr...| B07QZHQTVJ|         Whirlpool|Whirlpool W109185...|         Whirlpool|B07QZHQTVJ Tools ...|b07qzhqtvj tools ...|[b07qzhqtvj, tool...|[b07qzhqtvj, tool...|(3054,[0,1,2,3,40...|\n",
      "|[Appliances, Part...|[This is an After...|{NULL, NULL, NULL...|  [This is an After...|Tools & Home Impr...| B006R6N2WO|        FRIGIDAIRE|Front Drum Felt &...|             Supco|B006R6N2WO Tools ...|b006r6n2wo tools ...|[b006r6n2wo, tool...|[b006r6n2wo, tool...|(3054,[0,1,2,23,3...|\n",
      "|[Appliances, Part...|[Samsung Range/St...|{NULL, NULL, NULL...|  [Samsung ASSY VAL...|Tools & Home Impr...| B00YZ50ZTM|           SAMSUNG|Samsung DG94-0093...|           Samsung|B00YZ50ZTM Tools ...|b00yz50ztm tools ...|[b00yz50ztm, tool...|[b00yz50ztm, tool...|(3054,[0,1,2,9,14...|\n",
      "|[Appliances, Part...|                  []|{NULL, NULL, NULL...|  [COMPATIBLE REPLA...|         Amazon Home| B06XP2NFXL|             Tier1|Tier1 5 Micron 10...|             Tier1|B06XP2NFXL Amazon...|b06xp2nfxl amazon...|[b06xp2nfxl, amaz...|[b06xp2nfxl, amaz...|(3054,[0,4,7,8,11...|\n",
      "+--------------------+--------------------+--------------------+----------------------+--------------------+-----------+------------------+--------------------+------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tf_ctvector.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237f5398",
   "metadata": {},
   "source": [
    "Create IDF for each product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3dfce14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# The IDF class is used to compute the inverse document frequency (IDF) of the terms in the feature vectors.\n",
    "# The IDF component is used to down-weight the importance of common terms and up-weight the importance of rare terms.\n",
    "\n",
    "idf = IDF(inputCol=\"feat_vectors\", outputCol=\"feat_idf\")\n",
    "\n",
    "if tf_ctvector:\n",
    "        idf_model = idf.fit(tf_ctvector)\n",
    "        tfidf = idf_model.transform(tf_ctvector)\n",
    "elif tf_vector:\n",
    "        idf_model = idf.fit(tf_vector)\n",
    "        tfidf = idf_model.transform(tf_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5f2e389b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns in the tfidf vector \n",
    "cols_to_drop = ['feat_preproc', 'feat_tokens', 'feat_lemma', 'feat_vectors']\n",
    "tfidf = tfidf.drop(*cols_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96eafd1",
   "metadata": {},
   "source": [
    "Create a database view of the vectorized tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "40f2de10",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.createOrReplaceTempView(\"v_meta_tfidf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "df8c134e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+----------+--------------------+----------+--------------------+--------------------+\n",
      "|          categories|         description|             details|            features|       main_category|parent_asin|     store|               title|     maker|       feature_group|            feat_idf|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+----------+--------------------+----------+--------------------+--------------------+\n",
      "|[Appliances, Part...|                  []|{NULL, NULL, NULL...|                  []|Tools & Home Impr...| B07CPY3X2X|    UMTELE|Compatible with L...|    UMTELE|B07CPY3X2X Tools ...|(3054,[0,1,2,4,6,...|\n",
      "|[Appliances, Part...|[8268961 Dishwash...|{NULL, NULL, NULL...|[8268961 2pcs Dis...|Tools & Home Impr...| B09JYZZ8NQ|   GZsenwo|8268961 Dishwashe...|   GZsenwo|B09JYZZ8NQ Tools ...|(3054,[0,1,2,3,17...|\n",
      "|[Appliances, Part...|                  []|{NULL, NULL, NULL...|                  []|Tools & Home Impr...| B0824J3GGJ|        WY|Dryer Belt Replac...|        WY|B0824J3GGJ Tools ...|(3054,[0,1,2,3,7,...|\n",
      "|[Appliances, Part...|[Fit Models: RF22...|{NULL, NULL, NULL...|[DA97-12942A This...|Tools & Home Impr...| B09Y5T57KX|   TNITRIB|Optimize and Upgr...|   TNITRIB|B09Y5T57KX Tools ...|(3054,[0,1,2,11,1...|\n",
      "|[Appliances, Dish...|[Get better resul...|{NULL, NULL, NULL...|                  []|          Appliances| B0052EK0MW|KitchenAid|KitchenAid Superb...|KitchenAid|B0052EK0MW Applia...|(3054,[5,18,29,31...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+----------+--------------------+----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "v_meta_tfidf = spark.sql(\"SELECT * FROM v_meta_tfidf\")\n",
    "v_meta_tfidf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5106b7",
   "metadata": {},
   "source": [
    "##### Build query in SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "37048b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF: Query builder using SQL \n",
    "\n",
    "def query_builder(search_asin = None, search_category = None, search_maker = None, search_title = None):\n",
    "    \"\"\"\n",
    "    Build a SQL query to filter the DataFrame based on the provided search criteria.\n",
    "\n",
    "    Args:\n",
    "        search_category (str, optional): Category to search for. Defaults to None.\n",
    "        search_maker (str, optional): Maker to search for. Defaults to None.\n",
    "        search_title (str, optional): Title to search for. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        str: SQL query string.\n",
    "    \"\"\"\n",
    "\n",
    "    conditions = []\n",
    "    if search_asin:\n",
    "        conditions.append(f\" parent_asin = '{search_asin}'\")\n",
    "    if search_category:\n",
    "        conditions.append(f\" main_category LIKE '%{search_category}%'\")\n",
    "    if search_maker:\n",
    "        # conditions.append(f\" maker LIKE '%{search_maker}%'\")\n",
    "        conditions.append(f\" maker = '{search_maker}'\")\n",
    "    if search_title:\n",
    "        conditions.append(f\" title LIKE '%{search_title}%'\")\n",
    "    if not conditions:\n",
    "        raise ValueError(\"At least one search criterion must be provided.\")\n",
    "    \n",
    "    conditions_str = \" AND \".join(conditions)\n",
    "\n",
    "    query = f\"SELECT * FROM v_meta_tfidf WHERE ({conditions_str})\"\n",
    "    \n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "69957174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Try: Test Cases\n",
    "\n",
    "\n",
    "# # Test Case #1: Search by category and maker/brand\n",
    "# query_statement = query_builder(search_asin=None, search_category='Amazon Home', search_maker='Frigidaire' )\n",
    "\n",
    "# # Test Case #2: Seacrh by category, maker, and title\n",
    "# # query_statement = query_builder(search_asin= None, search_category='Electronics', search_maker='Samsung', search_title='DC66')\n",
    "\n",
    "# # print(query_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "153b1a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get results from sql query \n",
    "# query_results = spark.sql(query_statement)\n",
    "# print(f'Query results count: {query_results.count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe37b36",
   "metadata": {},
   "source": [
    "#### Transform Query Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b230dec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_query = [input('Enter Product To Search: ') ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9c8819a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frigidaire refrigirators\n"
     ]
    }
   ],
   "source": [
    "# Validate:\n",
    "input_query = 'Frigidaire refrigirators'\n",
    "print( input_query )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4ae25cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert string to spark dataframe\n",
    "input_df = spark.createDataFrame([(input_query,)], ['feat_preproc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "782855d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the input query\n",
    "query_pp = preprocess_text_udf(input_query)\n",
    "\n",
    "# Transform input query to a pyspark dataframe\n",
    "# query_pp_df = spark.createDataFrame([query_pp], StringType()).toDF('feat_preproc')\n",
    "query_pp_df = spark.createDataFrame([(input_query,)], ['feat_preproc'])\n",
    "\n",
    "# Tokenize the preprocessed text\n",
    "query_token = tokenizer.transform(query_pp_df)\n",
    "\n",
    "# Lemmatize the tokens\n",
    "query_lemma = query_token.withColumn('feat_lemma', lemmatize_tokens_udf(query_token['feat_tokens']))\n",
    "\n",
    "# Get term frequency vector for the lemmatized tokens\n",
    "query_tf = countvec_model.transform(query_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d7eb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|        feat_preproc|         feat_tokens|\n",
      "+--------------------+--------------------+\n",
      "|Frigidaire refrig...|[frigidaire, refr...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Verify\n",
    "# query_token.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0eb9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|        feat_preproc|         feat_tokens|          feat_lemma|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|Frigidaire refrig...|[frigidaire, refr...|[frigidaire, refr...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# # Verify\n",
    "# query_lemma.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "07377193",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-----------------+\n",
      "|        feat_preproc|         feat_tokens|          feat_lemma|     feat_vectors|\n",
      "+--------------------+--------------------+--------------------+-----------------+\n",
      "|Frigidaire refrig...|[frigidaire, refr...|[frigidaire, refr...|(3054,[23],[1.0])|\n",
      "+--------------------+--------------------+--------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# # Verify \n",
    "# query_tf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "041d8224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/edu/DataScience/CapstoneProjects/Spark_RecSys/venv_spark/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/edu/DataScience/CapstoneProjects/Spark_RecSys/venv_spark/lib/python3.12/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/edu/anaconda3/lib/python3.12/socket.py\", line 708, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "ERROR:root:Exception while sending command.                         (0 + 8) / 8]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/edu/DataScience/CapstoneProjects/Spark_RecSys/venv_spark/lib/python3.12/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/edu/DataScience/CapstoneProjects/Spark_RecSys/venv_spark/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/edu/DataScience/CapstoneProjects/Spark_RecSys/venv_spark/lib/python3.12/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Get the IDF vector for the term frequency vector\u001b[39;00m\n\u001b[32m      2\u001b[39m idf = IDF(inputCol=\u001b[33m\"\u001b[39m\u001b[33mfeat_vectors\u001b[39m\u001b[33m\"\u001b[39m, outputCol=\u001b[33m\"\u001b[39m\u001b[33mquery_idf\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m idf_model = \u001b[43midf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_tf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m query_tfidf = idf_model.transform(query_tf)\n\u001b[32m      6\u001b[39m query_tfidf.cache()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataScience/CapstoneProjects/Spark_RecSys/venv_spark/lib/python3.12/site-packages/pyspark/ml/base.py:205\u001b[39m, in \u001b[36mEstimator.fit\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    203\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy(params)._fit(dataset)\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    208\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    209\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m % \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[32m    210\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataScience/CapstoneProjects/Spark_RecSys/venv_spark/lib/python3.12/site-packages/pyspark/ml/wrapper.py:381\u001b[39m, in \u001b[36mJavaEstimator._fit\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    380\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) -> JM:\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m     java_model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    382\u001b[39m     model = \u001b[38;5;28mself\u001b[39m._create_model(java_model)\n\u001b[32m    383\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._copyValues(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataScience/CapstoneProjects/Spark_RecSys/venv_spark/lib/python3.12/site-packages/pyspark/ml/wrapper.py:378\u001b[39m, in \u001b[36mJavaEstimator._fit_java\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28mself\u001b[39m._transfer_params_to_java()\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_java_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataScience/CapstoneProjects/Spark_RecSys/venv_spark/lib/python3.12/site-packages/py4j/java_gateway.py:1321\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1314\u001b[39m args_command, temp_args = \u001b[38;5;28mself\u001b[39m._build_args(*args)\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m-> \u001b[39m\u001b[32m1321\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1322\u001b[39m return_value = get_return_value(\n\u001b[32m   1323\u001b[39m     answer, \u001b[38;5;28mself\u001b[39m.gateway_client, \u001b[38;5;28mself\u001b[39m.target_id, \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataScience/CapstoneProjects/Spark_RecSys/venv_spark/lib/python3.12/site-packages/py4j/java_gateway.py:1038\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1036\u001b[39m connection = \u001b[38;5;28mself\u001b[39m._get_connection()\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[32m   1040\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m._create_connection_guard(connection)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataScience/CapstoneProjects/Spark_RecSys/venv_spark/lib/python3.12/site-packages/py4j/clientserver.py:511\u001b[39m, in \u001b[36mClientServerConnection.send_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    510\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m         answer = smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:-\u001b[32m1\u001b[39m])\n\u001b[32m    512\u001b[39m         logger.debug(\u001b[33m\"\u001b[39m\u001b[33mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m\"\u001b[39m.format(answer))\n\u001b[32m    513\u001b[39m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[32m    514\u001b[39m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/lib/python3.12/socket.py:708\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    706\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    707\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    710\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Get the IDF vector for the term frequency vector\n",
    "idf = IDF(inputCol=\"feat_vectors\", outputCol=\"query_idf\")\n",
    "idf_model = idf.fit(query_tf)\n",
    "\n",
    "query_tfidf = idf_model.transform(query_tf)\n",
    "query_tfidf.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440e9c5d",
   "metadata": {},
   "source": [
    "### Calculate Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1c7d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crosstab to self?\n",
    "\n",
    "# tfidf = tfidf.crossJoin(tfidf.withColumnRenamed(\"feat_idf\", \"feat_idf2\"))\n",
    "# tfidf.show(2)\n",
    "# tfidf.select( 'feat_idf', 'feat_idf2').show(5, truncate=False)\n",
    "# similarity = tfidf.withColumn(\"cos_sim\", cos_sim(F.col('feat_idf'), F.col('feat_idf2')) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5d416a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF: Cosine Similarity\n",
    "\n",
    "# @F.udf(returnType=FloatType())\n",
    "def cos_sim(u, v):\n",
    "\n",
    "  return float(u.dot(v) / (u.norm(2) * v.norm(2)))\n",
    "\n",
    "# Register the UDF\n",
    "cos_sim_udf = F.udf(cos_sim, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92c5acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Verify\n",
    "# query_results.select(col('feat_idf')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818cda75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+-------+--------------------+-------+--------------------+--------------------+\n",
      "|          categories|         description|             details|            features|       main_category|parent_asin|  store|               title|  maker|       feature_group|            feat_idf|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+-------+--------------------+-------+--------------------+--------------------+\n",
      "|[Appliances, Part...|                  []|{NULL, NULL, NULL...|                  []|Tools & Home Impr...| B07CPY3X2X| UMTELE|Compatible with L...| UMTELE|B07CPY3X2X Tools ...|(3054,[0,1,2,4,6,...|\n",
      "|[Appliances, Part...|[8268961 Dishwash...|{NULL, NULL, NULL...|[8268961 2pcs Dis...|Tools & Home Impr...| B09JYZZ8NQ|GZsenwo|8268961 Dishwashe...|GZsenwo|B09JYZZ8NQ Tools ...|(3054,[0,1,2,3,17...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+-----------+-------+--------------------+-------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Verify\n",
    "# tfidf.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622d60df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+---------------------------+---------------------------+-----------------+-----------------+\n",
      "|feat_preproc            |feat_tokens                |feat_lemma                 |feat_vectors     |query_idf        |\n",
      "+------------------------+---------------------------+---------------------------+-----------------+-----------------+\n",
      "|Frigidaire refrigirators|[frigidaire, refrigirators]|[frigidaire, refrigirators]|(3054,[23],[1.0])|(3054,[23],[0.0])|\n",
      "+------------------------+---------------------------+---------------------------+-----------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# # Validate query results from previous call\n",
    "\n",
    "# print(query_tfidf.count())\n",
    "# query_tfidf.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7bd754",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.=====>                   (2 + 1) / 3]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/edu/DataScience/CapstoneProjects/Spark_RecSys/venv_spark/lib/python3.12/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/edu/DataScience/CapstoneProjects/Spark_RecSys/venv_spark/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/edu/DataScience/CapstoneProjects/Spark_RecSys/venv_spark/lib/python3.12/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/edu/DataScience/CapstoneProjects/Spark_RecSys/venv_spark/lib/python3.12/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/edu/DataScience/CapstoneProjects/Spark_RecSys/venv_spark/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/edu/DataScience/CapstoneProjects/Spark_RecSys/venv_spark/lib/python3.12/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataScience/CapstoneProjects/Spark_RecSys/venv_spark/lib/python3.12/site-packages/pyspark/sql/dataframe.py:1263\u001b[39m, in \u001b[36mDataFrame.collect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m._sc):\n\u001b[32m-> \u001b[39m\u001b[32m1263\u001b[39m     sock_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1264\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataScience/CapstoneProjects/Spark_RecSys/venv_spark/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataScience/CapstoneProjects/Spark_RecSys/venv_spark/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataScience/CapstoneProjects/Spark_RecSys/venv_spark/lib/python3.12/site-packages/py4j/protocol.py:334\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    335\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    336\u001b[39m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name))\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mPy4JError\u001b[39m: An error occurred while calling o390.collectToPython",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m q = \u001b[43mquery_tfidf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mquery_idf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(q)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataScience/CapstoneProjects/Spark_RecSys/venv_spark/lib/python3.12/site-packages/pyspark/sql/dataframe.py:2997\u001b[39m, in \u001b[36mDataFrame.first\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2977\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfirst\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Optional[Row]:\n\u001b[32m   2978\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Returns the first row as a :class:`Row`.\u001b[39;00m\n\u001b[32m   2979\u001b[39m \n\u001b[32m   2980\u001b[39m \u001b[33;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2995\u001b[39m \u001b[33;03m    Row(age=2, name='Alice')\u001b[39;00m\n\u001b[32m   2996\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2997\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataScience/CapstoneProjects/Spark_RecSys/venv_spark/lib/python3.12/site-packages/pyspark/sql/dataframe.py:2973\u001b[39m, in \u001b[36mDataFrame.head\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m   2941\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Returns the first ``n`` rows.\u001b[39;00m\n\u001b[32m   2942\u001b[39m \n\u001b[32m   2943\u001b[39m \u001b[33;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2970\u001b[39m \u001b[33;03m[Row(age=2, name='Alice')]\u001b[39;00m\n\u001b[32m   2971\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2972\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2973\u001b[39m     rs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2974\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m rs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2975\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.take(n)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataScience/CapstoneProjects/Spark_RecSys/venv_spark/lib/python3.12/site-packages/pyspark/sql/dataframe.py:2975\u001b[39m, in \u001b[36mDataFrame.head\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m   2973\u001b[39m     rs = \u001b[38;5;28mself\u001b[39m.head(\u001b[32m1\u001b[39m)\n\u001b[32m   2974\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m rs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2975\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataScience/CapstoneProjects/Spark_RecSys/venv_spark/lib/python3.12/site-packages/pyspark/sql/dataframe.py:1407\u001b[39m, in \u001b[36mDataFrame.take\u001b[39m\u001b[34m(self, num)\u001b[39m\n\u001b[32m   1378\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtake\u001b[39m(\u001b[38;5;28mself\u001b[39m, num: \u001b[38;5;28mint\u001b[39m) -> List[Row]:\n\u001b[32m   1379\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\u001b[39;00m\n\u001b[32m   1380\u001b[39m \n\u001b[32m   1381\u001b[39m \u001b[33;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1405\u001b[39m \u001b[33;03m    [Row(age=14, name='Tom'), Row(age=23, name='Alice')]\u001b[39;00m\n\u001b[32m   1406\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1407\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataScience/CapstoneProjects/Spark_RecSys/venv_spark/lib/python3.12/site-packages/pyspark/sql/dataframe.py:1262\u001b[39m, in \u001b[36mDataFrame.collect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcollect\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> List[Row]:\n\u001b[32m   1243\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[32m   1244\u001b[39m \n\u001b[32m   1245\u001b[39m \u001b[33;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1260\u001b[39m \u001b[33;03m    [Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[32m   1261\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1262\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSCCallSiteSync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1263\u001b[39m \u001b[43m        \u001b[49m\u001b[43msock_info\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1264\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataScience/CapstoneProjects/Spark_RecSys/venv_spark/lib/python3.12/site-packages/pyspark/traceback_utils.py:81\u001b[39m, in \u001b[36mSCCallSiteSync.__exit__\u001b[39m\u001b[34m(self, type, value, tb)\u001b[39m\n\u001b[32m     79\u001b[39m SCCallSiteSync._spark_stack_depth -= \u001b[32m1\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m SCCallSiteSync._spark_stack_depth == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jsc\u001b[49m\u001b[43m.\u001b[49m\u001b[43msetCallSite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataScience/CapstoneProjects/Spark_RecSys/venv_spark/lib/python3.12/site-packages/py4j/java_gateway.py:1321\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1314\u001b[39m args_command, temp_args = \u001b[38;5;28mself\u001b[39m._build_args(*args)\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m-> \u001b[39m\u001b[32m1321\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1322\u001b[39m return_value = get_return_value(\n\u001b[32m   1323\u001b[39m     answer, \u001b[38;5;28mself\u001b[39m.gateway_client, \u001b[38;5;28mself\u001b[39m.target_id, \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataScience/CapstoneProjects/Spark_RecSys/venv_spark/lib/python3.12/site-packages/py4j/java_gateway.py:1036\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry=\u001b[38;5;28;01mTrue\u001b[39;00m, binary=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1016\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[32m   1017\u001b[39m \u001b[33;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[32m   1018\u001b[39m \u001b[33;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1034\u001b[39m \u001b[33;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[32m   1035\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1038\u001b[39m         response = connection.send_command(command)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataScience/CapstoneProjects/Spark_RecSys/venv_spark/lib/python3.12/site-packages/py4j/clientserver.py:284\u001b[39m, in \u001b[36mJavaClient._get_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection.socket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataScience/CapstoneProjects/Spark_RecSys/venv_spark/lib/python3.12/site-packages/py4j/clientserver.py:291\u001b[39m, in \u001b[36mJavaClient._create_new_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    288\u001b[39m     connection = ClientServerConnection(\n\u001b[32m    289\u001b[39m         \u001b[38;5;28mself\u001b[39m.java_parameters, \u001b[38;5;28mself\u001b[39m.python_parameters,\n\u001b[32m    290\u001b[39m         \u001b[38;5;28mself\u001b[39m.gateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_thread_connection(connection)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataScience/CapstoneProjects/Spark_RecSys/venv_spark/lib/python3.12/site-packages/py4j/clientserver.py:438\u001b[39m, in \u001b[36mClientServerConnection.connect_to_java_server\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ssl_context:\n\u001b[32m    436\u001b[39m     \u001b[38;5;28mself\u001b[39m.socket = \u001b[38;5;28mself\u001b[39m.ssl_context.wrap_socket(\n\u001b[32m    437\u001b[39m         \u001b[38;5;28mself\u001b[39m.socket, server_hostname=\u001b[38;5;28mself\u001b[39m.java_address)\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28mself\u001b[39m.stream = \u001b[38;5;28mself\u001b[39m.socket.makefile(\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    440\u001b[39m \u001b[38;5;28mself\u001b[39m.is_connected = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "q = query_tfidf.select('query_idf').first()[0]\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cac3d4",
   "metadata": {},
   "source": [
    "<!-- compute cosine similarity between query_tfidf and  tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a4b1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SparseVector' object has no attribute '_get_object_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[76]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Compute cosine similarity between the tfidf vectors and the query results.\u001b[39;00m\n\u001b[32m      3\u001b[39m query_idf = query_tfidf.select(\u001b[33m'\u001b[39m\u001b[33mquery_idf\u001b[39m\u001b[33m'\u001b[39m).first()[\u001b[32m0\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m similarity = tfidf.withColumn(\u001b[33m\"\u001b[39m\u001b[33mcos_sim\u001b[39m\u001b[33m\"\u001b[39m, cos_sim_udf(F.col(\u001b[33m'\u001b[39m\u001b[33mfeat_idf\u001b[39m\u001b[33m'\u001b[39m), \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_idf\u001b[49m\u001b[43m)\u001b[49m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataScience/CapstoneProjects/Spark_RecSys/venv_spark/lib/python3.12/site-packages/pyspark/sql/utils.py:174\u001b[39m, in \u001b[36mtry_remote_functions.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    172\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(functions, f.\u001b[34m__name__\u001b[39m)(*args, **kwargs)\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataScience/CapstoneProjects/Spark_RecSys/venv_spark/lib/python3.12/site-packages/pyspark/sql/functions.py:193\u001b[39m, in \u001b[36mlit\u001b[39m\u001b[34m(col)\u001b[39m\n\u001b[32m    191\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    192\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _invoke_function(\u001b[33m\"\u001b[39m\u001b[33mlit\u001b[39m\u001b[33m\"\u001b[39m, col).astype(dt).alias(\u001b[38;5;28mstr\u001b[39m(col))\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_invoke_function\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataScience/CapstoneProjects/Spark_RecSys/venv_spark/lib/python3.12/site-packages/pyspark/sql/functions.py:97\u001b[39m, in \u001b[36m_invoke_function\u001b[39m\u001b[34m(name, *args)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     96\u001b[39m jf = _get_jvm_function(name, SparkContext._active_spark_context)\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Column(\u001b[43mjf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataScience/CapstoneProjects/Spark_RecSys/venv_spark/lib/python3.12/site-packages/py4j/java_gateway.py:1314\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1313\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args):\n\u001b[32m-> \u001b[39m\u001b[32m1314\u001b[39m     args_command, temp_args = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1316\u001b[39m     command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m         \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m         args_command +\\\n\u001b[32m   1319\u001b[39m         proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m     answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataScience/CapstoneProjects/Spark_RecSys/venv_spark/lib/python3.12/site-packages/py4j/java_gateway.py:1283\u001b[39m, in \u001b[36mJavaMember._build_args\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1279\u001b[39m     new_args = args\n\u001b[32m   1280\u001b[39m     temp_args = []\n\u001b[32m   1282\u001b[39m args_command = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(\n\u001b[32m-> \u001b[39m\u001b[32m1283\u001b[39m     [\u001b[43mget_command_part\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m new_args])\n\u001b[32m   1285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m args_command, temp_args\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DataScience/CapstoneProjects/Spark_RecSys/venv_spark/lib/python3.12/site-packages/py4j/protocol.py:298\u001b[39m, in \u001b[36mget_command_part\u001b[39m\u001b[34m(parameter, python_proxy_pool)\u001b[39m\n\u001b[32m    296\u001b[39m         command_part += \u001b[33m\"\u001b[39m\u001b[33m;\u001b[39m\u001b[33m\"\u001b[39m + interface\n\u001b[32m    297\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m298\u001b[39m     command_part = REFERENCE_TYPE + \u001b[43mparameter\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_object_id\u001b[49m()\n\u001b[32m    300\u001b[39m command_part += \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m command_part\n",
      "\u001b[31mAttributeError\u001b[39m: 'SparseVector' object has no attribute '_get_object_id'"
     ]
    }
   ],
   "source": [
    "# Compute cosine similarity between the tfidf vectors and the query results.\n",
    "\n",
    "query_idf = query_tfidf.select('query_idf').first()[0]\n",
    "similarity = tfidf.withColumn(\"cos_sim\", cos_sim_udf(F.col('feat_idf'), F.lit(query_idf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f1cbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the similarity results\n",
    "\n",
    "similarity.select('parent_asin', 'cos_sim', 'title', 'main_category', 'maker').show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f260504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation: Save similarity results to csv. \n",
    "# Toggle below codes to save using hasftf or countvectorizer.\n",
    "\n",
    "n_result = 20\n",
    "\n",
    "\n",
    "df_cos_sim = similarity.select('parent_asin', 'cos_sim', 'title', 'main_category', 'maker').toPandas().sort_values('cos_sim', ascending=False)\n",
    "df_cos_sim = df_cos_sim.head(n_result)\n",
    "\n",
    "\n",
    "if hashtf_file:\n",
    "    # similarity.select('parent_asin', 'cos_sim', 'title', 'main_category', 'maker').show(10, truncate=False).toPandas().sort_values('cos_sim', ascending=False).head(10)\n",
    "\n",
    "    df_cos_sim.to_csv(hashtf_file, index=False)\n",
    "\n",
    "elif ctvec_file:\n",
    "    \n",
    "    df_cos_sim.to_csv(ctvec_file, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feeb1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity.select('parent_asin', 'cos_sim', 'title', 'main_category', 'maker').show(10, truncate=False).toPandas().sort_values('cos_sim', ascending=False).head(10)\n",
    "# similarity.select('parent_asin', 'cos_sim', 'title', 'main_category', 'maker').toPandas().sort_values('cos_sim', ascending=False).to_csv('./data/similarity_hashtf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c3e1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad26907",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc67a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b4854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity = cosine_similarity(idfscores.select('feat_idf').rdd.map(lambda x: x[0]).collect())\n",
    "# similarity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0607ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create and fit the MinHashLSH model to the feature vectors\n",
    "# # Notes:\n",
    "# # The MinHashLSH -  creates a locality-sensitive hashing (LSH) model for approximate nearest neighbor search.\n",
    "# # numHashTable -  number of hash tables to use for the LSH model.\n",
    "# # Fitted hash results -  transforms the feature vectors into hash values.\n",
    "\n",
    "\n",
    "# mh = MinHashLSH(inputCol=\"idf\", outputCol=\"mh_hashes\", numHashTables=5)\n",
    "# mh_model = mh.fit(idfscores)\n",
    "\n",
    "# # Transform the feature data to include hash values\n",
    "# transformedData = mh_model.transform(idfscores)\n",
    "# transformedData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0884e096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aa9858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_meta.withColumn('c_brand', F.col('details').getItem('Brand')).show(5)\n",
    "# df_meta.withColumn('c_manufac', F.col('details').getItem('Manufacturer')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b051af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.withColumn('c_brand', F.col('details').getItem('Brand')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e803deb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd = spark.sparkContext.parallelize([jsonData])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
